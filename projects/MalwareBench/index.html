<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="A study on the security of large language models (LLMs) regarding malware requests and jailbreak attacks. It proposes the MalwareBench benchmark dataset, tests 29 mainstream models, reveals model security vulnerabilities, and provides directions for enhancing security research.">
  <meta property="og:title" content="Research on the Security Challenges of Large Language Models in Malware Requests and Jailbreak Attacks" />
  <meta property="og:description" content="This research focuses on the security issues of LLMs in malware - related tasks. By constructing the MalwareBench dataset and using multiple jailbreak attack methods to test the models, it analyzes their security performance and influencing factors, providing important references for subsequent improvements in model security." />
  <meta property="og:url" content="https://mail-tele-ai.github.io/projects/MalwareBench/index.html" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MalwareBench</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LLMs Caught in the Crossfire: Malware Requests and Jailbreak
              Challenges</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:haoyanglee@buaa.edu.cn" target="_blank">Haoyang Li</a><sup>+</sup>,</span>
              <span class="author-block">
                <a href="mailto:huangao@buaa.edu.cn" target="_blank">Huan Gao</a><sup>+</sup>,</span>
              <span class="author-block">
                <a href="mailto:tuzixini@gmail.com" target="_blank">Zhiyuan Zhao</a><sup>+</sup>,</span>
              <span class="author-block">
                <a href="mailto:zyllin@bjtu.edu.cn" target="_blank">Zhiyu Lin</a><sup>+</sup>,</span>
              <span class="author-block">
                <a href="mailto:gjy3035@gmail.com" target="_blank">Junyu Gao</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="mailto:xuelong_li@ieee.org" target="_blank">Xuelong Li</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institute of Artificial Intelligence (TeleAI), China Telecom<br>ACL 2025</span>
              <span class="eql-cntrb"><small><br><sup>+</sup>Indicates Equal Contribution</small></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Authors</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.10022" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/MAIL-Tele-AI/MalwareBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          Your video here
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The widespread adoption of Large Language Models (LLMs) has heightened concerns about their security, particularly their vulnerability to jailbreak attacks that leverage crafted prompts to generate malicious outputs. While prior research has been conducted on general security capabilities of LLMs, their specific susceptibility to jailbreak attacks in code generation remains largely unexplored. To fill this gap, we propose MalwareBench, a benchmark dataset containing 3,520 jailbreaking prompts for malicious code-generation, designed to evaluate LLM robustness against such threats. MalwareBench is based on 320 manually crafted malicious code generation requirements, covering 11 jailbreak methods and 29 code functionality categories. Experiments show that mainstream LLMs exhibit limited ability to reject malicious code-generation requirements, and the combination of multiple jailbreak methods further reduces the model's security capabilities: specifically, the average rejection rate for malicious content is 60.93\%, dropping to 39.92\% when combined with jailbreak attack algorithms. Our work highlights that the code security capabilities of LLMs still pose significant challenges.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/overview.png" alt="MY ALT TEXT" />
            <h2 class="subtitle">
              The paper "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges" delves into LLMs' 
              security in malware - related tasks. As LLMs are widely used in code generation but are vulnerable to 
              jailbreak attacks, and prior research lacks in - depth exploration in this area, the authors introduce 
              MalwareBench. This dataset contains 3,520 jailbreaking prompts based on 320 manually - crafted 
              requirements across multiple domains, sub - categories, and 11 black - box jailbreaking methods. 
              Experiments on 29 mainstream LLMs show that there's a negative correlation between response scores 
              and refusal rates. Different models react differently to attack algorithms, and LLMs perform variedly 
              for different requirement types. The study exposes the vulnerability of current LLMs to malicious code 
              attacks. Despite limitations such as single - model reliance for prompt generation and incomplete 
              requirement coverage, it offers insights for future research on enhancing LLM security. 
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/malware_overview.png" alt="MY ALT TEXT" />
            <h2 class="subtitle">
              When constructing MalwareBench, researchers referred to the malimg dataset to categorize malicious problems into 
              6 primary classifications based on user intent, with further secondary and tertiary classifications for some 
              categories. They manually crafted 5 - 20 malicious requirements for each detailed category, considering different 
              operating systems and dividing requirements into rough and detailed types. They also selected 11 jailbreak methods 
              of three types, using Qwen - Turbo in some for question generation. The 320 - question set was designed to assess 
              LLMs' resistance to generating malicious content. Initially, code - generation models had a 70.56% rejection rate 
              and generic large models had 51.19%, but these rates dropped to 51.50% and 41.47% respectively when jailbreak 
              methods were applied. The balanced rejection rate of the question set is essential for comprehensively evaluating 
              models' security in identifying malicious content. 
            </h2>
          </div>
          <!-- <div class="item">
            <img src="static/images/model_result_examples.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              The following content illustrates examples of how models can be applied to modify DNS records for malicious intent,
               along with responses from various language model algorithms. Several machine learning and deep learning models 
               were trained and evaluated on the dataset to classify malware types and predict potential attack vectors. Notably,
                transformer-based models showed superior performance in detecting polymorphic and metamorphic malware due to 
                their ability to capture complex patterns in obfuscated code. The figure below displays sample outputs generated
                 by these models when analyzing suspicious scripts and payloads.
            </h2>
          </div> -->
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/heat_map.png" alt="MY ALT TEXT" />
            <h2 class="subtitle">
              The study's results comprehensively analyze LLMs' performance. There's a negative correlation between response scores 
              and refusal rates; about 50.35% of jailbreak attempts yield malicious content, with OpenAI-o1 and CodeLlama-70B-Instruct 
              showing strong security. LLMs are more defensive against detailed requirements. Small parameter models give irrelevant 
              outputs, and large ones are more prone to malicious pseudo-code, relying on knowledge bases. Advanced reasoning models 
              like OpenAI-o1 and DeepSeek-R1 can handle malicious requests, but security alignment is crucial. Different models have 
              varied sensitivities to attack algorithms; Benign Expression is a highly effective jailbreaking method. Regarding 
              requirement types, LLMs show a consistent trend, with low scores for Denial Service and Download&Propagation, and 
              high scores for Information Theft. This may be due to training data or model mechanisms. Some models have security 
              alignment flaws, while CodeLlama-70B-Instruct has a strong defense, and Llama 3 series' safety measures might have 
              sacrificed safety for task performance. 
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->


  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Examples</h2>
          <img src="static/images/model_result_examples.png"/>
      </div>
    </div>
  </section> -->
  <section class="section" id="Example">
    <div class="container is-max-desktop content">
      <h2 class="title">Examples</h2>
        <img src="static/images/model_result_examples.png"/>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{li2025llmscaughtcrossfiremalware,
              title={LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges}, 
              author={Haoyang Li and Huan Gao and Zhiyuan Zhao and Zhiyu Lin and Junyu Gao and Xuelong Li},
              year={2025},
              eprint={2506.10022},
              archivePrefix={arXiv},
              primaryClass={cs.CR},
              url={https://arxiv.org/abs/2506.10022}, 
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              Powered by <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
              target="_blank">Academic Project Page Template</a> from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              <br>Licensed under <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>